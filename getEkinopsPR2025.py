import os, uuid, time, requests, pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin

COMPANY = "Ekinops"
BASE = "https://www.ekinops.com"
LIST_URL = f"{BASE}/news"
MASTER_FILE = os.path.join(os.path.dirname(__file__), "press_releases_master.csv")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0"
}

# ---------- CSV handling ----------
def load_master():
    if os.path.exists(MASTER_FILE):
        try:
            return pd.read_csv(MASTER_FILE, encoding="utf-8")
        except UnicodeDecodeError:
            return pd.read_csv(MASTER_FILE, encoding="latin-1")
    return pd.DataFrame(columns=["id","company","title","link","date","fetched_at"])

def save_to_master(df_new):
    if df_new.empty:
        print("‚ÑπÔ∏è No new press releases to add."); return
    master = load_master()
    if master.empty:
        updated = df_new
    else:
        merged = pd.merge(df_new, master[["company","title"]],
                          on=["company","title"], how="left", indicator=True)
        only_new = merged[merged["_merge"]=="left_only"].drop(columns="_merge")
        updated = pd.concat([master, only_new[df_new.columns]], ignore_index=True)
    updated.to_csv(MASTER_FILE, index=False, encoding="utf-8-sig")
    print(f"‚úÖ Added {len(df_new)} {COMPANY} press releases to {MASTER_FILE}")

# ---------- helpers ----------
def normalize_date(date_str):
    if not date_str:
        return ""
    try:
        return datetime.strptime(date_str.strip(), "%d %B %Y").strftime("%Y-%m-%d")
    except Exception:
        return date_str.strip()

def get_page(url):
    """Fetch a webpage with robust headers and retry logic to bypass 403s."""
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "DNT": "1",
    }

    for attempt in range(3):
        try:
            r = requests.get(url, headers=headers, timeout=30)
            if r.status_code == 403:
                print(f"‚ö†Ô∏è Attempt {attempt+1}: 403 Forbidden ‚Äî retrying with delay...")
                time.sleep(2 + attempt)
                continue
            r.raise_for_status()
            return BeautifulSoup(r.text, "html.parser")
        except requests.RequestException as e:
            print(f"‚ö†Ô∏è Attempt {attempt+1} failed: {e}")
            time.sleep(2)
    raise RuntimeError(f"‚ùå Unable to fetch {url} after 3 attempts")

# ---------- main ----------
def main():
    cutoff = datetime(2025, 1, 1)
    offset = 0
    step = 16
    rows = []
    seen_links = set()

    print(f"Fetching Ekinops press releases starting from {LIST_URL}")

    while True:
        url = f"{LIST_URL}?start={offset}" if offset else LIST_URL
        print(f"üåê Page {offset // step + 1}: {url}")
        soup = get_page(url)

        items = soup.select("div.sppb-article-info-wrap")
        if not items:
            print("‚ö†Ô∏è No more press releases found ‚Äî stopping.")
            break

        stop = False
        for item in items:
            a = item.select_one("h3 a[itemprop='url']")
            date_el = item.select_one("div.sppb-article-meta span.sppb-meta-date[itemprop='datePublished']")

            if not a or not a.get("href"):
                continue

            link = urljoin(BASE, a["href"])
            if link in seen_links:
                continue
            seen_links.add(link)

            title = a.get_text(strip=True)
            date_str = normalize_date(date_el.get_text(strip=True) if date_el else "")

            try:
                date_obj = datetime.strptime(date_str, "%Y-%m-%d")
            except Exception:
                date_obj = None

            # Stop once we reach a date before 2025
            if date_obj and date_obj < cutoff:
                print(f"üõë Reached {date_str} (<2025) ‚Äî stopping pagination.")
                stop = True
                break

            if date_obj and date_obj >= cutoff:
                rows.append({
                    "id": str(uuid.uuid4()),
                    "company": COMPANY,
                    "title": title,
                    "link": link,
                    "date": date_str,
                    "fetched_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
                print(f"üì∞ {title}\nüìÖ {date_str}\nüîó {link}\n")

            time.sleep(0.1)

        if stop:
            break
        offset += step
        time.sleep(0.7)

    df_new = pd.DataFrame(rows, columns=["id","company","title","link","date","fetched_at"])
    print(f"‚úÖ Total kept (2025+): {len(df_new)}")
    save_to_master(df_new)

if __name__ == "__main__":
    main()
